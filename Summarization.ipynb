{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from random import sample\n",
    "import re\n",
    "\n",
    "# for summarization\n",
    "from transformers import TFXLNetForSequenceClassification, XLNetTokenizer, T5Tokenizer, TFT5ForConditionalGeneration, PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from newspaper import Article, Config\n",
    "from heapq import nlargest\n",
    "from GoogleNews import GoogleNews\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# for partial matching strings\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# for document similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein-wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install google\n",
    "# !pip install newspaper3k\n",
    "# !pip install GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers  --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## allsides webscraping to get bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the news sources and biases in a dataframe: data\n",
    "data = pd.read_csv('./bias_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>website</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABC News (Online)</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlterNet</td>\n",
       "      <td>https://www.alternet.org</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>https://apnews.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Axios</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             source                   website    bias\n",
       "0           0  ABC News (Online)    https://abcnews.go.com    left\n",
       "1           1           AlterNet  https://www.alternet.org    left\n",
       "2           2   Associated Press        https://apnews.com  center\n",
       "3           3              Axios     https://www.axios.com  center\n",
       "4           4           BBC News       https://www.bbc.com  center"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Bias function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alternative_bias(article_bias):\n",
    "    \"\"\"Gets the other biases from the article bias\n",
    "    \n",
    "       input: string, the bias of the article - options: left, center, right\n",
    "       output: list, of the alternative biases. \n",
    "       eg. get_opposite_bias('right') returns ['left', 'center']\"\"\"\n",
    "    biases = ['left', 'center', 'right']\n",
    "    try:\n",
    "        biases.remove(article_bias)\n",
    "        \n",
    "    except ValueError:\n",
    "        # no bias, return list of just center\n",
    "        biases = ['center']\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at google/pegasus-xsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# pegasus model loaded\n",
    "pegasus_model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_summary(article_url):\n",
    "    \"\"\"\n",
    "    Creates a short summary of the article and gets the month and year article published\n",
    "    for the search query for related articles\n",
    "    \n",
    "   input:  string, url of the article\n",
    "   output: string, short summary of the article\n",
    "           string, month and year that the article was published in\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        txt = article.text\n",
    "        try:\n",
    "            pub_date = article.publish_date\n",
    "            month_yr = pub_date.strftime(\"%B\") + \" \" + str(pub_date.year)\n",
    "        except:\n",
    "            month_yr = \"\"\n",
    "            print('no published date')\n",
    "        pegasus_input = pegasus_tokenizer([txt], max_length=512, truncation=True, return_tensors='tf')\n",
    "        # max_length is 20 because google search only takes up to 32 words in one search \n",
    "        pegasus_summary_id =  pegasus_model.generate(pegasus_input['input_ids'], \n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=5,\n",
    "                                    max_length=29,\n",
    "                                    early_stopping=True)\n",
    "        pegasus_summary_ = [pegasus_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in pegasus_summary_id]\n",
    "        return pegasus_summary_[0], month_yr\n",
    "    except:\n",
    "        print('article not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Alternative Articles - combines output from short summary and alternative bias functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_alternative_articles(original_url, title, date, alternative_bias, bias_data):\n",
    "    \"\"\"\n",
    "    Gets the related alternative articles url links through google search\n",
    "    \n",
    "    input: original_url - string, url of the article that we are trying to get alternative articles for\n",
    "           title - string, short summary of article,\n",
    "           date - string, month and year,\n",
    "           alternative_bias - list of string, the alternative sides of bias of the article,\n",
    "           bias_data - dataframe - 2 columns, the source and the bias\n",
    "    output: list of tuples, first element of the tuple is the url of the alternative bias covering the same topic\n",
    "                            second element of the tuple is the source name\n",
    "   \"\"\"\n",
    "    # google news way\n",
    "#     articles = []\n",
    "#     sources = []\n",
    "#     googlenews = GoogleNews()\n",
    "#     for bias in alternative_bias: \n",
    "#         source_list = bias_data[bias_data.bias == bias].source.tolist()\n",
    "#         sample_sources = sample(source_list, 7)\n",
    "#         for source in sample_sources:\n",
    "#             query_list = [title, 'article', date, source]\n",
    "#             source_url = bias_data[bias_data.source == source].website.iloc[0]\n",
    "#             query = ' '.join(query_list)\n",
    "#             googlenews.search(query)\n",
    "#             news_links = googlenews.get_links()\n",
    "#             print(news_links)\n",
    "#             for article_url in news_links:\n",
    "#                 if article_url in articles or fuzz.partial_ratio(original_url, article_url) > 80:\n",
    "#                     continue\n",
    "#                 elif source_url not in article_url:\n",
    "#                     if fuzz.partial_ratio(source_url, article_url) > 80:\n",
    "#                         articles.append(article_url)\n",
    "#                         sources.append(source)\n",
    "#                         googlenews.clear()\n",
    "#                         break\n",
    "#                     else:\n",
    "#                             continue\n",
    "#                 else:\n",
    "#                     articles.append(article_url)\n",
    "#                     sources.append(source)\n",
    "#                     googlenews.clear()\n",
    "#                     break\n",
    "    # google search way        \n",
    "    articles = []\n",
    "    sources = []\n",
    "    for bias in alternative_bias: \n",
    "        source_list = bias_data[bias_data.bias == bias].source.tolist()\n",
    "        sample_sources = sample(source_list, 4)\n",
    "        for source in sample_sources:\n",
    "            query_list = [title, 'article', date, source]\n",
    "            source_url = bias_data[bias_data.source == source].website.iloc[0]\n",
    "            query = ' '.join(query_list)\n",
    "            search_generator = search(query, num = 2, pause = 3)\n",
    "            article_url = next(search_generator)\n",
    "            if article_url in articles or fuzz.partial_ratio(original_url, article_url) > 80:\n",
    "                article_url2 = next(search_generator)\n",
    "                if article_url2 in articles or fuzz.partial_ratio(original_url, article_url2) > 80:\n",
    "                    continue\n",
    "                elif fuzz.partial_ratio(source_url, article_url2) > 80:\n",
    "                    articles.append(article_url2)\n",
    "                    sources.append(source)\n",
    "            elif source_url not in article_url:\n",
    "                if fuzz.partial_ratio(source_url, article_url) > 80:\n",
    "                    articles.append(article_url)\n",
    "                    sources.append(source)\n",
    "                else:\n",
    "                    article_url2 = next(search_generator)\n",
    "                    if article_url2 in articles or fuzz.partial_ratio(original_url, article_url2) > 80:\n",
    "                        continue\n",
    "                    elif fuzz.partial_ratio(source_url, article_url2) > 80:\n",
    "                        articles.append(article_url2)\n",
    "                        sources.append(source)\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                articles.append(article_url)\n",
    "                sources.append(source)\n",
    "    zipped_list = list(zip(articles,sources))\n",
    "    return zipped_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Link of Alternative Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_info(zipped_urls_sources):\n",
    "    \"\"\"\n",
    "    Convert the urls and sources to the text and the titles of the related articles\n",
    "    \n",
    "    input: list, urls of related articles\n",
    "    output:list of tuples, first element of tuple is the article text\n",
    "                           second element of tuple is the article title\n",
    "                           third element of tuple is the source name\n",
    "    \"\"\"\n",
    "    article_texts = []\n",
    "    article_titles = []\n",
    "    article_sources = []\n",
    "    article_urls = []\n",
    "    urls, sources = zip(*zipped_urls_sources)\n",
    "    for index in range(len(urls)):\n",
    "        try:\n",
    "            article = Article(urls[index])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            txt = article.text\n",
    "            article.nlp()\n",
    "            # if there is no text in the article it isn't included\n",
    "            if txt:\n",
    "                # if there is less than 35 words in the article, it isn't included\n",
    "                if len(txt.split(' ')) < 35:\n",
    "                    continue\n",
    "                else:\n",
    "                    article_urls.append(urls[index])\n",
    "                    article_texts.append(txt)\n",
    "                    article_titles.append(article.title)\n",
    "                    article_sources.append(sources[index])\n",
    "        except:\n",
    "            continue\n",
    "    zipped_articles = list(zip(article_texts, article_titles, article_sources, article_urls))\n",
    "    return zipped_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Keep more similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_documents(articles):\n",
    "    \"\"\"\"\n",
    "    function to get similar documents in order to ensure we have the articles that have the same topic, event or issue\n",
    "    \n",
    "    input: list of tuples, article texts, titles, sources, urls\n",
    "    output: list of tuples, first element of the tuple are article texts that have high similarity to one another \n",
    "                            second element of the tuple are the titles of the articles\n",
    "    \"\"\"\n",
    "    texts, titles, sources, urls = zip(*articles)\n",
    "    tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "    pairwise_similarity = tfidf * tfidf.T\n",
    "    \n",
    "    # for each document compute the average similarity score to the other documents\n",
    "    # .53 is an arbitrary threshold\n",
    "    # should be higher than .53 average to make sure that the documents talk about the same topic\n",
    "    avg_similarity = np.average(pairwise_similarity.toarray(), axis = 1)\n",
    "    bool_similarity = avg_similarity > 0.53\n",
    "    # get the list of articles that fulfill the requirement of .55 avg similarity\n",
    "    \n",
    "    #if there are more than 4 articles that have greater than .45 similarities, only take the top 4 similarities \n",
    "    if sum(bool_similarity) > 4:\n",
    "        top_indexes = avg_similarity.argsort()[-4:][::-1]\n",
    "        updated_texts = [texts[i] for i in top_indexes]\n",
    "        updated_titles = [titles[i] for i in top_indexes]\n",
    "        updated_sources = [sources[i] for i in top_indexes]\n",
    "        updated_urls = [urls[i] for i in top_indexes]\n",
    "    elif sum(bool_similarity) == 0:\n",
    "        #if there is no article that has a collective similarity score\n",
    "        # print something\n",
    "        # returns null\n",
    "        print(\"No similar articles\")\n",
    "        return np.nan\n",
    "    else:\n",
    "        updated_texts = list(np.array(texts)[bool_similarity])\n",
    "        updated_titles = list(np.array(titles)[bool_similarity])\n",
    "        updated_sources = list(np.array(sources)[bool_similarity])\n",
    "        updated_urls = list(np.array(urls)[bool_similarity])\n",
    "    zipped_similar = list(zip(updated_texts, updated_titles, updated_sources, updated_urls))\n",
    "    return zipped_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization of similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load t5 model for second level summarization\n",
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(similar_articles_text):\n",
    "    \"\"\" \n",
    "    Summarize the article texts using pegasus model(abstractive) on each text and then combine the summaries into a string and\n",
    "    put it into the t5 model (extractive)\n",
    "    \n",
    "    input: tuple of similar article text\n",
    "    output: string of the summary of similar articles\n",
    "    \"\"\"\n",
    "    # summarize each article using pegasus\n",
    "    pegasus_input_list = [pegasus_tokenizer([text], max_length=512, truncation=True, return_tensors='tf')\n",
    "                          for text in similar_articles_text]\n",
    "    \n",
    "    pegasus_summary_ids =  [pegasus_model.generate(i['input_ids'], \n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=60,\n",
    "                                    max_length=300,\n",
    "                                    early_stopping=True) for i in pegasus_input_list]\n",
    "    \n",
    "    pegasus_summary_list = [[pegasus_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in i] for i in pegasus_summary_ids]\n",
    "    \n",
    "    # combine the pegasus summaries into a string\n",
    "    pegasus_summaries = \" \".join([i[0] for i in pegasus_summary_list])\n",
    "    \n",
    "    # get final summary through t5 model\n",
    "    total_input_list = t5_tokenizer([\"summarize: \" + pegasus_summaries], truncation = True, return_tensors = 'tf')\n",
    "    t5_id =  t5_model.generate(total_input_list['input_ids'],\n",
    "                                    num_beams=6,\n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=50,\n",
    "                                    max_length=300)\n",
    "    t5_summary = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in t5_id]\n",
    "    return t5_summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_url = 'https://www.foxnews.com/world/us-army-special-ops-veterans-take-matters-into-their-own-hands-to-get-trusted-ally-out-of-afghanistan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model output but right now manual output\n",
    "alt_biases = get_alternative_bias('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no published date\n"
     ]
    }
   ],
   "source": [
    "# get summary and date for input into link alternative article functions\n",
    "summary, date = short_summary(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A GoFundMe campaign has been launched to raise money for an Afghan special forces interpreter who has been unable to get a visa to come to the'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.bloomberg.com/news/articles/2021-06-24/biden-plans-to-relocate-afghans-who-helped-u-s-military-in-war',\n",
       "  'Bloomberg'),\n",
       " ('https://www.bbc.com/news/world-asia-56860781', 'BBC News'),\n",
       " ('https://apnews.com/article/government-and-politics-6dc242a6d170cfc419a09fb6ee0494db',\n",
       "  'Associated Press'),\n",
       " ('https://www.wsj.com/articles/afghan-translators-will-await-admission-to-u-s-in-other-countries-officials-say-11625757547',\n",
       "  'Wall Street Journal (News)')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google search results\n",
    "link_alternative_articles(article_url, summary, date, alt_biases, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alt_articles_links = link_alternative_articles(article_url, summary, date, alt_biases, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.npr.org/2021/06/19/1004991965/afghan-interpreters-who-await-visas-after-helping-the-u-s-now-fear-for-their-liv',\n",
       "  'NPR (Opinion) '),\n",
       " ('https://www.wsj.com/articles/afghan-translators-will-await-admission-to-u-s-in-other-countries-officials-say-11625757547',\n",
       "  'Wall Street Journal (News)'),\n",
       " ('https://www.bbc.com/news/world-asia-56860781', 'BBC News')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_articles_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unpack urls\n",
    "alt_articles = url_to_info(alt_articles_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep similar articles\n",
    "zipped_similar = similar_documents(alt_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, title, sources, urls= zip(*zipped_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Afghan Interpreters Who Await Visas After Helping The U.S. Now Fear For Their Lives',\n",
       " 'Afghanistan War: How can the West fight terrorism after leaving?')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NPR (Opinion) ', 'BBC News')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.npr.org/2021/06/19/1004991965/afghan-interpreters-who-await-visas-after-helping-the-u-s-now-fear-for-their-liv',\n",
       " 'https://www.bbc.com/news/world-asia-56860781')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarization(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as the u.s. prepares to withdraw its troops from Afghanistan next month, some Afghans who have worked for the united states say they fear for their lives . drone strikes against so-called Islamic state (IS) in Syria and Iraq have become a regular feature of military operations in the middle east . but their use has come under increasing scrutiny in recent months, particularly in the wake of the killing of british aid worker Alan Henning in a drone strike .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, sources = link_alternative_articles('https://www.nytimes.com/live/2021/06/24/us/joe-biden-news','biden infrastructure bill', 'June 2021',['right', 'center'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://nypost.com/2021/06/27/romney-has-faith-biden-will-sign-infrastructure-bill/',\n",
       " 'https://spectator.org/infrastructure-insanity-democrats/',\n",
       " 'https://www.dailywire.com/news/biden-backtrack-president-now-says-he-will-not-veto-infrastructure-deal',\n",
       " 'https://dailycaller.com/2021/06/29/joe-manchin-supports-democrat-only-joe-biden-infrastructure-bill/',\n",
       " 'https://www.usatoday.com/story/news/politics/2021/06/24/biden-senators-agree-1-2-trillion-infrastructure-deal/5333841001/',\n",
       " 'https://www.csmonitor.com/USA/Politics/2021/0629/Biden-wants-infrastructure.-Does-America-know-how-to-do-it-anymore',\n",
       " 'https://www.forbes.com/sites/arielcohen/2021/07/06/what-the-bipartisan-infrastructure-plan-means-for-us-energy/',\n",
       " 'https://www.wsj.com/articles/biden-needs-to-save-the-infrastructure-bill-11625608639']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York Post (News)',\n",
       " 'The American Spectator',\n",
       " 'The Daily Wire',\n",
       " 'The Daily Caller',\n",
       " 'USA TODAY',\n",
       " 'Christian Science Monitor',\n",
       " 'Forbes',\n",
       " 'Wall Street Journal (News)']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 functions for summarization\n",
    "\n",
    "1. summarization of article - input an article - output a pegasus sumarization of article - up to one sentence, month and year\n",
    "\n",
    "2. summarization of similar articles - input urls of articles - pegasus or newspaper (extractive) of articles - get document similarity to make sure talking about the same subject/event/issue, then pegasus & T5 sumarize the summaries to get a general overview of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
