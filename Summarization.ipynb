{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import time\n",
    "\n",
    "# for summarization\n",
    "from transformers import TFXLNetForSequenceClassification, XLNetTokenizer, T5Tokenizer, TFT5ForConditionalGeneration, PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "import datetime\n",
    "from newspaper import Article, Config\n",
    "from heapq import nlargest\n",
    "# from GoogleNews import GoogleNews\n",
    "from googlesearch import search\n",
    "\n",
    "# for partial matching strings\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# for document similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein-wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install googlesearch-python\n",
    "# !pip install newspaper3k\n",
    "# !pip install GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers  --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## allsides webscraping to get bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the news sources and biases in a dataframe: data\n",
    "data = pd.read_csv('./bias_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>website</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABC News (Online)</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlterNet</td>\n",
       "      <td>https://www.alternet.org</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>https://apnews.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Axios</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             source                   website    bias\n",
       "0           0  ABC News (Online)    https://abcnews.go.com    left\n",
       "1           1           AlterNet  https://www.alternet.org    left\n",
       "2           2   Associated Press        https://apnews.com  center\n",
       "3           3              Axios     https://www.axios.com  center\n",
       "4           4           BBC News       https://www.bbc.com  center"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Bias function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alternative_bias(article_bias):\n",
    "    \"\"\"Gets the other biases from the article bias\n",
    "    \n",
    "       input: string, the bias of the article - options: left, center, right\n",
    "       output: list, of the alternative biases. \n",
    "       eg. get_opposite_bias('right') returns ['left', 'center']\"\"\"\n",
    "    biases = ['left', 'center', 'right']\n",
    "    try:\n",
    "        biases.remove(article_bias)\n",
    "        \n",
    "    except ValueError:\n",
    "        # no bias, return list of just center\n",
    "        biases = ['center']\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at google/pegasus-xsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# pegasus model loaded\n",
    "pegasus_model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_summary(article_url):\n",
    "    \"\"\"\n",
    "    Creates a short summary of the article and gets the month and year article published\n",
    "    for the search query for related articles\n",
    "    \n",
    "   input:  string, url of the article\n",
    "   output: string, short summary of the article\n",
    "           string, month and year that the article was published in\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        txt = article.text\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Downloaded the article in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "        try:\n",
    "            pub_date = article.publish_date\n",
    "            month_yr = pub_date.strftime(\"%B\") + \" \" + str(pub_date.year)\n",
    "        except:\n",
    "            month_yr = \"\"\n",
    "            print('no published date')\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        pegasus_input = pegasus_tokenizer([txt], max_length=512, truncation=True, return_tensors='tf')\n",
    "        # max_length is 20 because google search only takes up to 32 words in one search \n",
    "        pegasus_summary_id =  pegasus_model.generate(pegasus_input['input_ids'], \n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=5,\n",
    "                                    max_length=29,\n",
    "                                    early_stopping=True)\n",
    "        pegasus_summary_ = [pegasus_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in pegasus_summary_id]\n",
    "        toc = time.perf_counter()\n",
    "\n",
    "        print(f\"Created the summary in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "        return pegasus_summary_[0], month_yr\n",
    "    except Exception as inst:\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)\n",
    "        print('article not found')\n",
    "        raise inst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Alternative Articles - combines output from short summary and alternative bias functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_alternative_articles(original_url, title, date, alternative_bias, bias_data):\n",
    "    \"\"\"\n",
    "    Gets the related alternative articles url links through google search\n",
    "    \n",
    "    input: original_url - string, url of the article that we are trying to get alternative articles for\n",
    "           title - string, short summary of article,\n",
    "           date - string, month and year,\n",
    "           alternative_bias - list of string, the alternative sides of bias of the article,\n",
    "           bias_data - dataframe - 2 columns, the source and the bias\n",
    "    output: list of tuples, first element of the tuple is the url of the alternative bias covering the same topic\n",
    "                            second element of the tuple is the source name\n",
    "   \"\"\"\n",
    "    # google news way\n",
    "#     articles = []\n",
    "#     sources = []\n",
    "#     googlenews = GoogleNews()\n",
    "#     for bias in alternative_bias: \n",
    "#         source_list = bias_data[bias_data.bias == bias].source.tolist()\n",
    "#         sample_sources = sample(source_list, 7)\n",
    "#         for source in sample_sources:\n",
    "#             query_list = [title, 'article', date, source]\n",
    "#             source_url = bias_data[bias_data.source == source].website.iloc[0]\n",
    "#             query = ' '.join(query_list)\n",
    "#             googlenews.search(query)\n",
    "#             news_links = googlenews.get_links()\n",
    "#             print(news_links)\n",
    "#             for article_url in news_links:\n",
    "#                 if article_url in articles or fuzz.partial_ratio(original_url, article_url) > 80:\n",
    "#                     continue\n",
    "#                 elif source_url not in article_url:\n",
    "#                     if fuzz.partial_ratio(source_url, article_url) > 80:\n",
    "#                         articles.append(article_url)\n",
    "#                         sources.append(source)\n",
    "#                         googlenews.clear()\n",
    "#                         break\n",
    "#                     else:\n",
    "#                             continue\n",
    "#                 else:\n",
    "#                     articles.append(article_url)\n",
    "#                     sources.append(source)\n",
    "#                     googlenews.clear()\n",
    "#                     break\n",
    "    # google search way        \n",
    "    articles = []\n",
    "    sources = []\n",
    "    for bias in alternative_bias: \n",
    "        source_list = bias_data[bias_data.bias == bias].source.tolist()\n",
    "        sample_sources = sample(source_list, 5)\n",
    "        for source in sample_sources:\n",
    "            print(source)\n",
    "            query_list = [title, 'article', date, source]\n",
    "            source_url = bias_data[bias_data.source == source].website.iloc[0]\n",
    "            query = ' '.join(query_list)\n",
    "            search_generator = search(query, num = 2, pause = 3)\n",
    "            article_url = next(search_generator)\n",
    "            if article_url in articles or fuzz.partial_ratio(original_url, article_url) > 80:\n",
    "                article_url2 = next(search_generator)\n",
    "                if article_url2 in articles or fuzz.partial_ratio(original_url, article_url2) > 80:\n",
    "                    continue\n",
    "                elif fuzz.partial_ratio(source_url, article_url2) > 80:\n",
    "                    articles.append(article_url2)\n",
    "                    sources.append(source)\n",
    "            elif source_url not in article_url:\n",
    "                if fuzz.partial_ratio(source_url, article_url) > 80:\n",
    "                    articles.append(article_url)\n",
    "                    sources.append(source)\n",
    "                else:\n",
    "                    article_url2 = next(search_generator)\n",
    "                    if article_url2 in articles or fuzz.partial_ratio(original_url, article_url2) > 80:\n",
    "                        continue\n",
    "                    elif fuzz.partial_ratio(source_url, article_url2) > 80:\n",
    "                        articles.append(article_url2)\n",
    "                        sources.append(source)\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                articles.append(article_url)\n",
    "                sources.append(source)\n",
    "    zipped_list = list(zip(articles,sources))\n",
    "    return zipped_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Link of Alternative Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_info(zipped_urls_sources):\n",
    "    \"\"\"\n",
    "    Convert the urls and sources to the text and the titles of the related articles\n",
    "    \n",
    "    input: list, urls of related articles\n",
    "    output:list of tuples, first element of tuple is the article text\n",
    "                           second element of tuple is the article title\n",
    "                           third element of tuple is the source name\n",
    "    \"\"\"\n",
    "    article_texts = []\n",
    "    article_titles = []\n",
    "    article_sources = []\n",
    "    article_urls = []\n",
    "    urls, sources = zip(*zipped_urls_sources)\n",
    "    for index in range(len(urls)):\n",
    "        try:\n",
    "            article = Article(urls[index])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            txt = article.text\n",
    "            article.nlp()\n",
    "            # if there is no text in the article it isn't included\n",
    "            if txt:\n",
    "                # if there is less than 35 words in the article, it isn't included\n",
    "                if len(txt.split(' ')) < 35:\n",
    "                    continue\n",
    "                else:\n",
    "                    article_urls.append(urls[index])\n",
    "                    article_texts.append(txt)\n",
    "                    article_titles.append(article.title)\n",
    "                    article_sources.append(sources[index])\n",
    "        except:\n",
    "            continue\n",
    "    zipped_articles = list(zip(article_texts, article_titles, article_sources, article_urls))\n",
    "    return zipped_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Keep more similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_documents(articles):\n",
    "    \"\"\"\"\n",
    "    function to get similar documents in order to ensure we have the articles that have the same topic, event or issue\n",
    "    \n",
    "    input: list of tuples, article texts, titles, sources, urls\n",
    "    output: list of tuples, first element of the tuple are article texts that have high similarity to one another \n",
    "                            second element of the tuple are the titles of the articles\n",
    "    \"\"\"\n",
    "    texts, titles, sources, urls = zip(*articles)\n",
    "    tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "    pairwise_similarity = tfidf * tfidf.T\n",
    "    \n",
    "    # for each document compute the average similarity score to the other documents\n",
    "    # .53 is an arbitrary threshold\n",
    "    # should be higher than .53 average to make sure that the documents talk about the same topic\n",
    "    avg_similarity = np.average(pairwise_similarity.toarray(), axis = 1)\n",
    "    bool_similarity = avg_similarity > 0.53\n",
    "    # get the list of articles that fulfill the requirement of .53 avg similarity\n",
    "    \n",
    "    #if there are more than 4 articles that have greater than .53 similarities, only take the top 4 similarities \n",
    "    if sum(bool_similarity) > 4:\n",
    "        top_indexes = avg_similarity.argsort()[-4:][::-1]\n",
    "        updated_texts = [texts[i] for i in top_indexes]\n",
    "        updated_titles = [titles[i] for i in top_indexes]\n",
    "        updated_sources = [sources[i] for i in top_indexes]\n",
    "        updated_urls = [urls[i] for i in top_indexes]\n",
    "    elif sum(bool_similarity) <=1:\n",
    "        #if there is less than 2 articles that has a collective similarity score over .53 \n",
    "        raise IndexError('No similar articles found')\n",
    "    else:\n",
    "        updated_texts = list(np.array(texts)[bool_similarity])\n",
    "        updated_titles = list(np.array(titles)[bool_similarity])\n",
    "        updated_sources = list(np.array(sources)[bool_similarity])\n",
    "        updated_urls = list(np.array(urls)[bool_similarity])\n",
    "    zipped_similar = list(zip(updated_texts, updated_titles, updated_sources, updated_urls))\n",
    "    return zipped_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization of similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load t5 model for second level summarization\n",
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(similar_articles_text):\n",
    "    \"\"\" \n",
    "    Summarize the article texts using pegasus model(abstractive) on each text and then combine the summaries into a string and\n",
    "    put it into the t5 model (extractive)\n",
    "    \n",
    "    input: tuple of similar article text\n",
    "    output: string of the summary of similar articles\n",
    "    \"\"\"\n",
    "    # summarize each article using pegasus\n",
    "    tic = time.perf_counter()\n",
    "    pegasus_input_list = [pegasus_tokenizer([text], max_length=512, truncation=True, return_tensors='tf')\n",
    "                          for text in similar_articles_text]\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Got the pegasus input list in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "    pegasus_summary_ids =  [pegasus_model.generate(i['input_ids'], \n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=60,\n",
    "                                    max_length=300,\n",
    "                                    early_stopping=True) for i in pegasus_input_list]\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    print(f\"Got the summary ids in {tic - toc:0.4f} seconds\")\n",
    "\n",
    "\n",
    "    pegasus_summary_list = [[pegasus_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in i] for i in pegasus_summary_ids]\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(f\"Got the summary ids in {toc - tic:0.4f} seconds\")\n",
    "    print(pegasus_summary_list)\n",
    "\n",
    "    # combine the pegasus summaries into a string\n",
    "    pegasus_summaries = \" \".join([i[0] for i in pegasus_summary_list])\n",
    "    \n",
    "    # pegasus summary for second round code\n",
    "#     pegasus_input2 = pegasus_tokenizer([pegasus_summaries], max_length=512, truncation=True, return_tensors='tf')\n",
    "#     tic = time.perf_counter()\n",
    "#     print(f\"Got the inputs for final pegasus run in {tic - toc:0.4f} seconds\")\n",
    "\n",
    "#     pegasus_summary_id2 =  pegasus_model.generate(pegasus_input2['input_ids'], \n",
    "#                                     no_repeat_ngram_size=5,\n",
    "#                                     min_length=60,\n",
    "#                                     max_length=300,\n",
    "#                                     early_stopping=True)\n",
    "#     toc = time.perf_counter()\n",
    "\n",
    "#     print(f\"Got the summary ids for pegasus2 in {toc - tic:0.4f} seconds\")\n",
    "#     pegasus_summary_2 = [pegasus_tokenizer.decode(g, skip_special_tokens=True, \n",
    "#                            clean_up_tokenization_spaces=False) for g in pegasus_summary_id2]\n",
    "\n",
    "#     tic = time.perf_counter()\n",
    "#     print(f\"Got the final pegasus summary in {tic - toc:0.4f} seconds\")\n",
    "#     return pegasus_summary_2[0]\n",
    "\n",
    "    # get final summary through t5 model\n",
    "    total_input_list = t5_tokenizer([\"summarize: \" + pegasus_summaries], truncation = True, return_tensors = 'tf')\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Got the input list for t5 in {tic - toc:0.4f} seconds\")\n",
    "    t5_id =  t5_model.generate(total_input_list['input_ids'],\n",
    "                                    num_beams=6,\n",
    "                                    no_repeat_ngram_size=5,\n",
    "                                    min_length=50,\n",
    "                                    max_length=300)\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Got the t5 id in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "    t5_summary = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in t5_id]\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Got the summary for t5 in {tic - toc:0.4f} seconds\")\n",
    "\n",
    "    return t5_summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_url = 'https://www.nytimes.com/2021/03/16/us/politics/election-interference-russia-2020-assessment.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model output but right now manual output\n",
    "alt_biases = get_alternative_bias('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the article in 0.5357 seconds\n",
      "Created the summary in 52.3849 seconds\n"
     ]
    }
   ],
   "source": [
    "# get summary and date for input into link alternative article functions\n",
    "summary, date = short_summary(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The US intelligence community has declassified a report on Russia’s efforts to meddle in the 2016 election, laying out how former Vice'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuffPost\n",
      "Bloomberg\n",
      "Time Magazine\n",
      "Democracy Now\n",
      "The New Yorker\n",
      "Christian Science Monitor\n",
      "Axios\n",
      "NPR (Online News)\n",
      "Associated Press\n",
      "Reuters\n"
     ]
    }
   ],
   "source": [
    "alt_articles_links = link_alternative_articles(article_url, summary, date, alt_biases, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.huffpost.com/news/topic/election-interference', 'HuffPost'),\n",
       " ('https://www.bloomberg.com/news/articles/2021-03-16/u-s-spy-agency-rejects-trump-claim-of-china-election-meddling',\n",
       "  'Bloomberg'),\n",
       " ('https://www.newyorker.com/news/news-desk/russias-election-meddling-is-another-american-intelligence-failure',\n",
       "  'The New Yorker'),\n",
       " ('https://www.csmonitor.com/USA/Politics/2019/0514/Amid-growing-concerns-about-2020-a-primer-on-Russian-election-interference',\n",
       "  'Christian Science Monitor'),\n",
       " ('https://www.axios.com/senate-gop-house-republicans-russia-meddling-e4f46fc6-3ec0-49c4-b719-0efc79a0a558.html',\n",
       "  'Axios'),\n",
       " ('https://www.npr.org/2020/08/18/903616280/sen-mark-warner-discusses-latest-report-on-russias-influence-on-2016-election',\n",
       "  'NPR (Online News)'),\n",
       " ('https://apnews.com/article/fact-checking-afs:Content:9458967050',\n",
       "  'Associated Press'),\n",
       " ('https://www.reuters.com/article/us-usa-trump-russia-q-a/what-we-know-about-u-s-probes-of-russian-meddling-in-2016-election-idUSKBN19604O',\n",
       "  'Reuters')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_articles_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unpack urls\n",
    "alt_articles = url_to_info(alt_articles_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep similar articles\n",
    "zipped_similar = similar_documents(alt_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, title, sources, urls= zip(*zipped_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Russia’s Election Meddling Was Another U.S. Intelligence Failure',\n",
       " 'Amid growing concerns about 2020, a primer on Russian election interference',\n",
       " 'What we know about U.S. probes of Russian meddling in 2016 election',\n",
       " 'Intel in letter is unverified, doesn’t show Clinton planned ‘Russia hoax’')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The New Yorker', 'Christian Science Monitor', 'Reuters', 'Associated Press')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.newyorker.com/news/news-desk/russias-election-meddling-is-another-american-intelligence-failure',\n",
       " 'https://www.csmonitor.com/USA/Politics/2019/0514/Amid-growing-concerns-about-2020-a-primer-on-Russian-election-interference',\n",
       " 'https://www.reuters.com/article/us-usa-trump-russia-q-a/what-we-know-about-u-s-probes-of-russian-meddling-in-2016-election-idUSKBN19604O',\n",
       " 'https://apnews.com/article/fact-checking-afs:Content:9458967050')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the pegasus input list in 0.0490 seconds\n",
      "Got the summary ids in 416.9655 seconds\n",
      "Got the summary ids in 0.0732 seconds\n",
      "[['For the first time since the Cold War, the United States has publicly admitted that Russia meddled in the 2016 presidential election in order to help Donald Trump and hurt his Democratic opponent, Hillary Clinton, by damaging their election and undermining public faith in the American political system, according to a new report.'], ['With new details from the Mueller report about Russia’s interference in the 2016 presidential election, an overview of what we know about what happened last time around and what the United States is doing to prevent another such attack, here’s a new sense of urgency around the question of what the U.S.'], ['Here is a guide to the controversy surrounding alleged Russian meddling in the 2016 presidential election and possible ties between President Donald Trump’s campaign team and Moscow, as well as the various investigations into the matter. (This article has been revised to reflect that the special counsel has been appointed, not that the Justice Department has appointed a special counsel.'], ['A look at some of the claims made in a series of letters sent by top Republicans and Democrats to the White House last week about alleged Russian interference in last year’s election and President Donald Trump’s links to the Kremlin, as reported by the Associated Press and other news outlets on Tuesday.']]\n",
      "Got the input list for t5 in 0.0037 seconds\n",
      "Got the t5 id in 90.4831 seconds\n",
      "Got the summary for t5 in 0.0792 seconds\n"
     ]
    }
   ],
   "source": [
    "summary = summarization(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the u.s. has publicly admitted that russia meddled in the 2016 presidential election . for the first time since the cold war, the united states has publicly admitted this, a new report says . here is a guide to the controversy surrounding alleged Russian meddling in the 2016 election .'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 functions for summarization\n",
    "\n",
    "1. summarization of article - input an article - output a pegasus sumarization of article - up to one sentence, month and year\n",
    "\n",
    "2. summarization of similar articles - input urls of articles - pegasus or newspaper (extractive) of articles - get document similarity to make sure talking about the same subject/event/issue, then pegasus & T5 sumarize the summaries to get a general overview of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/articles/mcconnell-threatens-pileup-if-democrats-change-filibuster-rules-11615908931 on URL https://www.wsj.com/articles/mcconnell-threatens-pileup-if-democrats-change-filibuster-rules-11615908931",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-3665178d7bcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\newspaper\\article.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow_if_not_downloaded_verbose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\newspaper\\article.py\u001b[0m in \u001b[0;36mthrow_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             raise ArticleException('Article `download()` failed with %s on URL %s' %\n\u001b[1;32m--> 532\u001b[1;33m                   (self.download_exception_msg, self.url))\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mthrow_if_not_parsed_verbose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArticleException\u001b[0m: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/articles/mcconnell-threatens-pileup-if-democrats-change-filibuster-rules-11615908931 on URL https://www.wsj.com/articles/mcconnell-threatens-pileup-if-democrats-change-filibuster-rules-11615908931"
     ]
    }
   ],
   "source": [
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        txt = article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speculation that President Joe Biden and the Democratic National Committee are petitioning cellphone carriers to monitor and edit private text messages is false, DNC and White House officials tell the Washington Examiner.\\n\\nStill, political operatives from both parties have made a practice of monitoring mass text blasts from the other side in recent years as SMS marketing has exploded as a tool for political campaigns, fundraising groups, and even private sector businesses.\\n\\nThe notion of new Democratic-backed SMS surveillance popped up in conservative circles after Politico published an article Monday that included a line that the DNC and other Biden-allied groups were \"planning to engage fact-checkers more aggressively and work with SMS carriers to dispel misinformation about vaccines that is sent over social media and text messages.\"\\n\\n\"The goal is to ensure that people who may have difficulty getting a vaccination because of issues like transportation see those barriers lessened or removed entirely,\" the article read.\\n\\nTRUMP CAMPAIGN FUNDRAISING THREATENED BY SPAM TEXTING ALLEGATIONS\\n\\nA DNC official told the Washington Examiner that the Democratic Party has been training its grassroots volunteers to sign up for various mass email lists from Republican-affiliated groups since 2019 and to flag noteworthy messages for the communications team.\\n\\nSimilarly, a Republican digital campaigns expert also confirmed that GOP operatives and volunteers frequently sign up to receive text messages from Democratic campaigns and affiliated groups in order to stay one step ahead of the competition.\\n\\nReferencing Politico\\'s report, the DNC official conceded that they have added another internal step to their SMS process as a means of combating vaccine misinformation but emphatically stated that the DNC has not and will not lobby SMS carriers, such as AT&T, T-Mobile, or Verizon, to monitor any text messages distributed at both the bulk list or peer-to-peer level.\\n\\nP2P messaging refers to any text conversation exchanged between two people. Bulk list messaging requires recipients to opt-in to receive communications from an automated sender, such as the Biden campaign\\'s \"30330\" and Trump campaign\\'s \"88022\" blasts or even online retailers.\\n\\nSenders may seek to grow their own reach by either building those lists organically or purchasing numbers from lists owned by other groups or private communications companies.\\n\\nHOUSE GOP DIGITAL FUNDRAISING ARM OUTRAISES DEMOCRATS FOR FIRST TIME\\n\\nThe DNC official said that Politico \"didn\\'t do a great job\" and \"fueled a lot of embellishment and speculation\" about the new initiative. That new process sees communications officials take the misleading bulk texts about vaccines flagged by volunteers and then forward them to SMS aggregator companies, such as Twilio or Bandwidth, \"who either work with a mass texting client or have companies that work with mass texting clients,\" the official said.\\n\\nThose aggregator companies all have \"fair use and abuse\" policies but \"almost all of the time\" have no visibility on what their clients are actually sending, \"so all we do is say, \\'Hey, did you know that you were sending out these messages?\\'\" the official added.\\n\\n\"The idea that, like, Joe Biden is reading everyone\\'s text messages, that\\'s not what happening,\" the DNC official said.\\n\\nSources on both sides noted that federal law prohibits SMS carriers from monitoring P2P messaging without a warrant, although carriers may deploy machine learning technology to block some bulk messages that violate fair use practices from being delivered.\\n\\nWhite House officials also told the Washington Examiner that though some administration officials, such as White House press secretary Jen Psaki and Dr. Anthony Fauci, have ramped up efforts to publicly dispute vaccine misinformation, the administration itself has nothing to do with the DNC initiative.\\n\\n\"We are steadfastly committed to keeping politics out of the effort to get every American vaccinated so that we can save lives and help our economy further recover,\" White House spokesperson Kevin Munoz previously told Politico. \"When we see deliberate efforts to spread misinformation, we view that as an impediment to the country\\'s public health and will not shy away from calling that out.\"\\n\\nConservatives reacted with ire on social media to Politico\\'s report, and some right-leaning television programs told their viewers that the president and his party were working to monitor their private text messages.\\n\\nYOUNG ADULTS SANK BIDEN\\'S VACCINE GOAL. HERE\\'S HOW THE WHITE HOUSE PLANS TO GET SHOTS IN THEIR ARMS\\n\\n\"Biden’s regime has announced they’ll be working with SMS providers to stop vaccine \\'misinformation\\' spread via text messages,\" freshman Colorado Republican Rep. Lauren Boebert tweeted. \"This is on the same day the White House said they support local officials who implement mandatory vaccines. No wonder they can’t condemn Communism.\"\\n\\nBiden’s regime has announced they’ll be working with SMS providers to stop vaccine “misinformation” spread via text messages.\\n\\n\\n\\nThis is on the same day the White House said they support local officials who implement mandatory vaccines.\\n\\n\\n\\nNo wonder they can’t condemn Communism. — Lauren Boebert (@laurenboebert) July 12, 2021\\n\\nMissouri Republican Sen. Josh Hawley suggested that the misinformation campaign was an effort to \"force vaccine compliance and who knows what else.\"\\n\\nCLICK HERE TO READ MORE FROM THE WASHINGTON EXAMINER\\n\\nSo now the Biden Administration wants to get into people’s text messages … to force vaccine compliance and who knows what else https://t.co/Q1v1qkOOfB — Josh Hawley (@HawleyMO) July 13, 2021\\n\\n\"Yes, we can allay the concerns of the excessively paranoid by tracking the content they send via text message,\" Noah Rothman, an editor at Commentary, wrote.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tic = time.perf_counter()\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Downloaded the tutorial in {toc - tic:0.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
